{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a33034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TrainSetGen as trainingSG\n",
    "import TestingSetGen as testingSG\n",
    "\n",
    "[train_label, train_coords_A, train_coords_B] = trainingSG.getPoints()\n",
    "[test_label, test_coords_A, test_coords_B] = testingSG.getPoints()\n",
    "\n",
    "# import numpy as np\n",
    "# train_data_coords, train_coords_A, train_coords_B\n",
    "# test_data_coords, test_coords_A, test_coords_B\n",
    "# np.shape(train_coords_A)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efde990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Common as cm\n",
    "\n",
    "[train_w, train_doppler] = cm.getWAndDoppler(train_coords_A, train_coords_B)\n",
    "[test_w, test_doppler] = cm.getWAndDoppler(test_coords_A, test_coords_B)\n",
    "\n",
    "# train_w, train_doppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728d86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Config as cf\n",
    "\n",
    "vertex1 = np.array([0, 0])\n",
    "vertex2 = np.array([200, 0])\n",
    "vertex3 = np.array([100, 100])\n",
    "vertex4 = np.array([300, 150])\n",
    "\n",
    "vertices_position = np.concatenate((vertex1, vertex2, vertex3, vertex4))\n",
    "        \n",
    "train_position = np.full((cf.train_points_num, len(vertices_position)), vertices_position)\n",
    "test_position = np.full((cf.test_points_num, len(vertices_position)), vertices_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60ffbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_w, test_doppler, test_position\n",
    "# np.shape(test_w), np.shape(test_doppler), np.shape(test_position)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd13f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# def pointInTriangle(pt, v1, v2, v3):\n",
    "#     # Barycentric coordinates method\n",
    "#     def sign(p1, p2, p3):\n",
    "#         return (p1[0] - p3[0]) * (p2[1] - p3[1]) - (p2[0] - p3[0]) * (p1[1] - p3[1])\n",
    "\n",
    "#     b1 = sign(pt, v1, v2) < 0.0\n",
    "#     b2 = sign(pt, v2, v3) < 0.0\n",
    "#     b3 = sign(pt, v3, v1) < 0.0\n",
    "\n",
    "#     return ((b1 == b2) and (b2 == b3))\n",
    "\n",
    "# def triangleConstraintLoss(outputs, targets, v1, v2, v3):\n",
    "#     criterion = nn.MSELoss()\n",
    "    \n",
    "#     # 计算常规损失\n",
    "#     mse_loss = criterion(outputs, targets)\n",
    "    \n",
    "#     # 计算三角形约束损失\n",
    "#     constraint_loss = 0.0\n",
    "#     for output in outputs:\n",
    "#         # 如果点不在三角形内，增加损失\n",
    "#         if not pointInTriangle(output, v1, v2, v3):\n",
    "#             constraint_loss += 100.0\n",
    "\n",
    "#     # 总损失是常规损失和约束损失的和\n",
    "#     total_loss = mse_loss + constraint_loss\n",
    "#     return total_loss\n",
    "\n",
    "# def triangleConstraintLoss(outputs, targets, v1, v2, v3):\n",
    "#     criterion = nn.MSELoss()\n",
    "    \n",
    "#     print(outputs.shape)\n",
    "    \n",
    "#     # 计算常规损失\n",
    "#     mse_loss = criterion(outputs, targets)\n",
    "    \n",
    "#     # # 计算三角形约束损失\n",
    "#     # constraint_loss = 0.0\n",
    "#     # for output in outputs:\n",
    "#     #     # 如果点不在三角形内，增加损失\n",
    "#     #     if not pointInTriangle(output, v1, v2, v3):\n",
    "#     #         constraint_loss += 100.0\n",
    "\n",
    "#     # # 总损失是常规损失和约束损失的和\n",
    "#     # total_loss = mse_loss + constraint_loss\n",
    "\n",
    "#     d11 = outputs[0]\n",
    "#     abs()\n",
    "#     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18aefa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.concatenate((train_w, train_doppler, train_position), axis=1)  # 将 相位差和多普勒 拼接\n",
    "test_inputs = np.concatenate((test_w, test_doppler, test_position), axis=1)  # 将 相位差和多普勒 拼接\n",
    "\n",
    "def normalize_first_n_columns(data, n):\n",
    "   # 计算前n列的最大值和最小值\n",
    "   min_vals = data[:, :n].min(axis=0)\n",
    "   max_vals = data[:, :n].max(axis=0)\n",
    "\n",
    "   # 归一化前n列\n",
    "   data[:, :n] = (data[:, :n] - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "\n",
    "# normalize the first 6 columns\n",
    "# normalize_first_n_columns(train_inputs, 6)\n",
    "# normalize_first_n_columns(test_inputs, 6)\n",
    "\n",
    "# train_other_inputs = train_data_coords\n",
    "# test_other_inputs = test_data_coords\n",
    "\n",
    "# import numpy as np\n",
    "# train_other_inputs.fill(0)\n",
    "# test_other_inputs.fill(0)\n",
    "\n",
    "# train_other_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e001988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(param), param\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 损失函数和优化器s\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    259\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# 定义更深的神经网络模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import Config as cf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "import dnn.RBFModel as RBFModel\n",
    "import Config as cf\n",
    "\n",
    "# 数据预处理\n",
    "# train_other_inputs = np.concatenate((train_w, train_doppler, train_position), axis=1)  # 将 相位差和多普勒 拼接\n",
    "# test_other_inputs = np.concatenate((test_w, test_doppler, test_position), axis=1)  # 将 相位差和多普勒 拼接\n",
    "\n",
    "# inputs_train, inputs_test, coords_train, coords_test = train_test_split(inputs, train_data_coords, test_size=0.1, random_state=42)\n",
    "\n",
    "# 定义一个 PyTorch 数据集\n",
    "class CoordDataset(Dataset):\n",
    "    def __init__(self, inputs, coords):\n",
    "        # 确保 inputs 和 coords 都是 torch.float32 类型\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.coords = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.coords[idx]\n",
    "    \n",
    "# 创建训练和测试数据集\n",
    "train_dataset = CoordDataset(train_inputs, train_label)\n",
    "test_dataset = CoordDataset(test_inputs, test_label)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=cf.train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cf.test_batch_size, shuffle=False)\n",
    "\n",
    "# 使用GPU加速\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RBFModel.RBF().to(device)\n",
    "\n",
    "\n",
    "# 损失函数和优化器s\n",
    "optimizer = optim.Adam(model.parameters(), lr=cf.learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 训练模型\n",
    "loss_values = []  # 存储损失值以供绘图\n",
    "\n",
    "for epoch in range(cf.epoch):\n",
    "    for inputs, coords in train_loader:\n",
    "        inputs, coords = inputs.to(device), coords.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, coords)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # 动态绘图\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(loss_values, label=\"Loss\")\n",
    "    plt.title(\"RBF Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 打印损失信息\n",
    "    print(f\"Epoch [{epoch+1}/{cf.epoch}], Loss: {loss.item()}\")\n",
    "\n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    predicted_coords = []\n",
    "    true_coords = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, coords in test_loader:\n",
    "            inputs, coords = inputs.to(device), coords.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, coords)\n",
    "            total_test_loss += test_loss.item()\n",
    "            predicted_coords.append(outputs.cpu().numpy())\n",
    "            true_coords.append(coords.cpu().numpy())\n",
    "\n",
    "    # 合并所有批次的预测结果和真实值\n",
    "    predicted_coords = np.concatenate(predicted_coords, axis=0)\n",
    "    true_coords = np.concatenate(true_coords, axis=0)\n",
    "    # 绘制散点图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        true_coords[:, 0],\n",
    "        true_coords[:, 1],\n",
    "        label=\"True Coordinates\",\n",
    "        marker=\"o\",\n",
    "        s=30,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        predicted_coords[:, 0],\n",
    "        predicted_coords[:, 1],\n",
    "        label=\"Predicted Coordinates\",\n",
    "        marker=\"x\",\n",
    "        s=30,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.xlabel(\"X-coordinate\")\n",
    "    plt.ylabel(\"Y-coordinate\")\n",
    "    plt.legend()\n",
    "    plt.title(\"True vs. Predicted Coordinates\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(f\"Test Loss: {total_test_loss / len(test_loader)}\")\n",
    "# print(\"Testing...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5ef3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6731b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试模型\n",
    "# model.eval()\n",
    "# total_test_loss = 0\n",
    "# predicted_coords = []\n",
    "# true_coords = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, coords in test_loader:\n",
    "#         inputs, coords = inputs.to(device), coords.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         test_loss = triangleConstraintLoss(outputs, coords, vertex1, vertex2, vertex3)\n",
    "#         total_test_loss += test_loss.item()\n",
    "#         predicted_coords.append(outputs.cpu().numpy())\n",
    "#         true_coords.append(coords.cpu().numpy())\n",
    "\n",
    "# # 合并所有批次的预测结果和真实值\n",
    "# predicted_coords = np.concatenate(predicted_coords, axis=0)\n",
    "# true_coords = np.concatenate(true_coords, axis=0)\n",
    "# # 绘制散点图\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(true_coords[:, 0], true_coords[:, 1], label='True Coordinates', marker='o', s=30, alpha=0.7)\n",
    "# plt.scatter(predicted_coords[:, 0], predicted_coords[:, 1], label='Predicted Coordinates', marker='x', s=30, alpha=0.7)\n",
    "# plt.xlabel('X-coordinate')\n",
    "# plt.ylabel('Y-coordinate')\n",
    "# plt.legend()\n",
    "# plt.title('True vs. Predicted Coordinates')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# print(f'Test Loss: {total_test_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f86ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(true_coords[:, 0], true_coords[:, 1], label='True Coordinates', marker='o', s=30, alpha=0.7)\n",
    "plt.scatter(predicted_coords[:, 0], predicted_coords[:, 1], label='Predicted Coordinates', marker='x', s=30, alpha=0.7)\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Coordinates')\n",
    "plt.grid(True)\n",
    "plt.xlim(65,115)\n",
    "plt.show()\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acb6c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "inputs = np.concatenate((train_w, v), axis=1)  # 将 w 和 v 拼接\n",
    "inputs_train, inputs_test, coords_train, coords_test = train_test_split(inputs, train_label, test_size=0.5, random_state=42)\n",
    "\n",
    "# inputs_test = inputs_t\n",
    "# coords_test = result_test\n",
    "\n",
    "# 定义一个 PyTorch 数据集\n",
    "# 定义一个 PyTorch 数据集\n",
    "class CoordDataset(Dataset):\n",
    "    def __init__(self, inputs, coords):\n",
    "        # 确保 inputs 和 coords 都是 torch.float32 类型\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.coords = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.coords[idx]\n",
    "# 创建训练和测试数据集\n",
    "train_dataset = CoordDataset(inputs_train, coords_train)\n",
    "test_dataset = CoordDataset(inputs_test, coords_test)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 定义更深的神经网络模型\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)  # 新增层\n",
    "        self.fc5 = nn.Linear(512, 512)  # 新增层\n",
    "        self.fc6 = nn.Linear(512, 256)\n",
    "        self.fc7 = nn.Linear(256, 128)\n",
    "        self.fc8 = nn.Linear(128, 64)\n",
    "        self.fc9 = nn.Linear(64, 32)\n",
    "        self.fc10 = nn.Linear(32, 4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))  # 新增层\n",
    "        x = self.relu(self.fc5(x))  # 新增层\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.relu(self.fc7(x))\n",
    "        x = self.relu(self.fc8(x))\n",
    "        x = self.relu(self.fc9(x))\n",
    "        x = self.fc10(x)\n",
    "        return x\n",
    "\n",
    "# 使用GPU加速\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNN().to(device)\n",
    "\n",
    "#考虑约束关系\n",
    "#两个接收机的圆相交点\n",
    "#计算A点和B点的距离差\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[\n",
    "    \n",
    "]\n",
    "#分段训练 10000个点 收敛 学习率降低 步长变短 \n",
    "#增加数据点\n",
    "\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "# 训练模型\n",
    "cf.epoch = 1500\n",
    "for epoch in range(cf.epoch):\n",
    "    for inputs, coords in train_loader:\n",
    "        inputs, coords = inputs.to(device), coords.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, coords)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{cf.epoch}], Loss: {loss.item()}')\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "predicted_coords = []\n",
    "true_coords = []\n",
    "with torch.no_grad():\n",
    "    for inputs, coords in test_loader:\n",
    "        inputs, coords = inputs.to(device), coords.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss = criterion(outputs, coords)\n",
    "        total_test_loss += test_loss.item()\n",
    "        predicted_coords.append(outputs.cpu().numpy())\n",
    "        true_coords.append(coords.cpu().numpy())\n",
    "\n",
    "# 合并所有批次的预测结果和真实值\n",
    "predicted_coords = np.concatenate(predicted_coords, axis=0)\n",
    "true_coords = np.concatenate(true_coords, axis=0)\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(true_coords[:, 0], true_coords[:, 1], label='True Coordinates', marker='o', s=30, alpha=0.7)\n",
    "plt.scatter(predicted_coords[:, 0], predicted_coords[:, 1], label='Predicted Coordinates', marker='x', s=30, alpha=0.7)\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Coordinates')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(true_coords[:1497, 0], true_coords[:1497, 1], label='True Coordinates', marker='o', s=30, alpha=0.7)\n",
    "plt.scatter(predicted_coords[:1497, 0], predicted_coords[:1497, 1], label='Predicted Coordinates', marker='x', s=30, alpha=0.7)\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "plt.legend()\n",
    "plt.title('True vs. Predicted Coordinates')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f'Test Loss: {total_test_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 0\n",
    "d2 = 0\n",
    "for i in range(len(true_coords)-1):\n",
    "    x = true_coords[i, 0]\n",
    "    y = true_coords[i, 1]\n",
    "    x1 = predicted_coords[i, 0]\n",
    "    y1 = predicted_coords[i, 1]\n",
    "    d11 = calculate_distance(x,y,vertex1[0],vertex1[1])\n",
    "    d12 = calculate_distance(x1,y1,vertex1[0],vertex1[1])\n",
    "    d21 = calculate_distance(x,y,vertex2[0],vertex2[1])\n",
    "    d22 = calculate_distance(x1,y1,vertex2[0],vertex2[1])\n",
    "    d31 = calculate_distance(x,y,vertex3[0],vertex3[1])\n",
    "    d32 = calculate_distance(x1,y1,vertex3[0],vertex3[1])\n",
    "    dd12 = np.abs(d11+d21-d12-d22)\n",
    "    dd13 = np.abs(d11+d31-d12-d32)\n",
    "    d1 = d1+dd12\n",
    "    d2 = d2+dd13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2/len(true_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f399137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
